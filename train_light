from common.utils import HDF5Dataset
from torch.utils.data import DataLoader
from experiments.models_gpt import GPT
from common.utils import GraphCreator
from equations.PDEs import *
import random
from experiments.models_gnn import MP_PDE_Solver
import wandb
from experiments.train_helper import *
from datetime import datetime
import torch_geometric

def train(gnn_config: dict,
          pde: PDE,
          epoch: int,
          model_gnn: torch.nn.Module,
          optimizer_gnn: torch.optim,
          loader: DataLoader,
          graph_creator: GraphCreator,
          criterion: torch.nn.modules.loss,
          model_gpt: torch.nn.Module = None,
          optimizer_gpt: torch.optim = None,
          scheduler_gpt: torch.optim.lr_scheduler = None,
          device: torch.cuda.device="cpu") -> None:
    """
    Training loop.
    Loop is over the mini-batches and for every batch we pick a random timestep.
    This is done for the number of timesteps in our training sample, which covers a whole episode.
    Args:
        args (argparse): command line inputs
        pde (PDE): PDE at hand [CE, WE, ...]
        model (torch.nn.Module): neural network PDE solver
        optimizer (torch.optim): optimizer used for training
        loader (DataLoader): training dataloader
        graph_creator (GraphCreator): helper object to handle graph data
        criterion (torch.nn.modules.loss): criterion for training
        device (torch.cuda.device): device (cpu/gpu)
    Returns:
        None
    """
    print(f'Starting epoch {epoch}...')
    print(f"Epoch {epoch}")
    if(use_gpt):
        model_gpt.train()
    model_gnn.train()

    max_unrolling = epoch if epoch <= gnn_config["unrolling"] else gnn_config["unrolling"]
    unrolling = [r for r in range(max_unrolling + 1)]

    # Loop over every epoch as often as the number of timesteps in one trajectory.
    # Since the starting point is randomly drawn, this in expectation has every possible starting point/sample combination of the training data.
    # Therefore in expectation the whole available training information is covered.
    for i in range(graph_creator.t_res):
        losses, norms_gnn, norms_gpt, lr_gpt = training_loop(model_gnn, unrolling, gnn_config["batch_size"], optimizer_gnn, loader, graph_creator, criterion, model_gpt, optimizer_gpt, scheduler_gpt, device)
        print(f'Training Loss (progress: {i / graph_creator.t_res:.2f}): {torch.mean(losses)}')
        wandb.log({"train/loss": torch.mean(losses),
                    "metrics/grad_gnn_norm": torch.mean(norms_gnn),
                    "metrics/grad_gpt_norm": torch.mean(norms_gpt),
                    "metrics/lr_gpt": lr_gpt,})

def test(gnn_config: dict,
         pde: PDE,
         model_gnn: torch.nn.Module,
         loader: DataLoader,
         graph_creator: GraphCreator,
         criterion: torch.nn.modules.loss,
         model_gpt: torch.nn.Module = None,
         device: torch.cuda.device="cpu") -> torch.Tensor:
    """
    Test routine
    Both step wise and unrolled forward losses are computed
    and compared against low resolution solvers
    step wise = loss for one neural network forward pass at certain timepoints
    unrolled forward loss = unrolling of the whole trajectory
    Args:
        args (argparse): command line inputs
        pde (PDE): PDE at hand [CE, WE, ...]
        model (torch.nn.Module): neural network PDE solver
        loader (DataLoader): dataloader [valid, test]
        graph_creator (GraphCreator): helper object to handle graph data
        criterion (torch.nn.modules.loss): criterion for training
        device (torch.cuda.device): device (cpu/gpu)
    Returns:
        torch.Tensor: unrolled forward loss
    """
    model_gnn.eval()
    if(use_gpt):
        model_gpt.eval()

   # first we check the losses for different timesteps (one forward prediction array!)
    steps = [t for t in range(graph_creator.tw, graph_creator.t_res-graph_creator.tw + 1)]
    losses = test_timestep_losses(model_gnn=model_gnn,
                                  steps=steps,
                                  batch_size=gnn_config["batch_size"],
                                  loader=loader,
                                  graph_creator=graph_creator,
                                  criterion=criterion,
                                  model_gpt=model_gpt,
                                  device=device)

    # next we test the unrolled losses
    losses = test_unrolled_losses(model_gnn=model_gnn,
                                  steps=steps,
                                  batch_size=gnn_config["batch_size"],
                                  nr_gt_steps=2,
                                  nx_base_resolution=gnn_config["base_resolution"][1],
                                  loader=loader,
                                  graph_creator=graph_creator,
                                  criterion=criterion,
                                  model_gpt= model_gpt,
                                  device=device)

    return torch.mean(losses)


gpt_config = {
        ## GPT Config 
        "use_gpt": True,
        "max_iters": 20,
        "batch_size" : 16, # if gradient_accumulation_steps > 1, this is the micro-batch size
        "n_x": 100, 
        "block_size" : 250, # max sequence length for predicting next timestep. Sequence length can be shorter.
        "n_layer" : 3, 
        "n_head" : 8,
        "n_embd" : 128,
        "n_gnn": 100,
        "dropout" : 0.0, # for pretraining 0 is good, for finetuning try 0.1+
        "bias" : False, # do we use bias inside LayerNorm and Linear layers?
        "learning_rate": 1e-3,
        "pct_start": 0.1,
        "min_lr": 1e-4, # should be lr/10 ish
        "beta1" : 0.9,
        "beta2" : 0.95,
        "grad_clip": 1.0,
        "device": "cuda",

}

gnn_config = {
        ## GNN Config 

        "epochs": gpt_config["max_iters"],
        "batch_size": gpt_config["batch_size"],
        "n_x": gpt_config["n_x"],
        "experiment": "E1",
        "base_resolution": (250, 100),
        "super_resolution": (250, 200),
        "neighbors": 3,
        "time_window": 25,
        "lr": 1e-4,
        "lr_decay": 0.6,
        "unrolling": 1,
        "device": gpt_config["device"],
        "compile": False,
}

device = gpt_config["device"]
use_gpt = gpt_config["use_gpt"]

config = gpt_config | gnn_config

run = wandb.init(
    project="gpt-mp-pde-solver",
    config=config)

## GNN Initialization
pde = CE(device=device)
train_string = f'data/{pde}_train_{gnn_config["experiment"]}.h5'
valid_string = f'data/{pde}_valid_{gnn_config["experiment"]}.h5'
test_string = f'data/{pde}_test_{gnn_config["experiment"]}.h5'

train_dataset = HDF5Dataset(train_string, pde=pde, mode='train', base_resolution=gnn_config["base_resolution"], super_resolution=gnn_config["super_resolution"])
train_loader = DataLoader(train_dataset,
                        batch_size=gnn_config["batch_size"],
                        shuffle=True,
                        num_workers=4)

valid_dataset = HDF5Dataset(valid_string, pde=pde, mode='valid', base_resolution=gnn_config["base_resolution"], super_resolution=gnn_config["super_resolution"])
valid_loader = DataLoader(valid_dataset,
                            batch_size=gnn_config["batch_size"],
                            shuffle=False,
                            num_workers=4)

test_dataset = HDF5Dataset(test_string, pde=pde, mode='test', base_resolution=gnn_config["base_resolution"], super_resolution=gnn_config["super_resolution"])
test_loader = DataLoader(test_dataset,
                            batch_size=gnn_config["batch_size"],
                            shuffle=False,
                            num_workers=4)

pde.tmin = train_dataset.tmin
pde.tmax = train_dataset.tmax
pde.grid_size = gnn_config["base_resolution"]
pde.dt = train_dataset.dt

eq_variables = {}
if gnn_config["experiment"] == 'E2':
        print('Beta parameter added to the GNN solver')
        eq_variables['beta'] = 0.2
elif gnn_config["experiment"]  == 'E3':
        print('Alpha, beta, and gamma parameter added to the GNN solver')
        eq_variables['alpha'] = 3.
        eq_variables['beta'] = 0.4
        eq_variables['gamma'] = 1.
elif (gnn_config["experiment"]  == 'WE3'):
        print('Boundary parameters added to the GNN solver')
        eq_variables['bc_left'] = 1
        eq_variables['bc_right'] = 1

graph_creator = GraphCreator(   pde=pde,
                                neighbors=gnn_config["neighbors"],
                                time_window=gnn_config["time_window"],
                                t_resolution=gnn_config["base_resolution"][0],
                                x_resolution=gnn_config["base_resolution"][1],
                                use_gpt=use_gpt).to(device)

model_gnn = MP_PDE_Solver(pde=pde,
                        time_window=graph_creator.tw,
                        eq_variables=eq_variables,
                        use_gpt=use_gpt)

model_parameters = filter(lambda p: p.requires_grad, model_gnn.parameters())
params = sum([np.prod(p.size()) for p in model_parameters])
print(f'Number of GNN parameters: {params}')
## GPT Initialization

if(use_gpt):
    model_gpt = GPT(gpt_config)
else:
    model_gpt = None
    optimizer_gpt = None
    scheduler_gpt = None

## Multi-GPU/Compile

if torch.cuda.device_count() > 1:
  print("Let's use", torch.cuda.device_count(), "GPUs!")
  model_gnn = nn.DataParallel(model_gnn)
  if(use_gpt):
    model_gpt = nn.DataParallel(model_gpt)

model_gnn = model_gnn.to(device)
optimizer_gnn = torch.optim.AdamW(model_gnn.parameters(), lr=gnn_config["lr"], fused=True)
scheduler_gnn = torch.optim.lr_scheduler.MultiStepLR(optimizer_gnn, milestones=[gnn_config["unrolling"], 5, 10, 15], gamma=gnn_config["lr_decay"])

if(use_gpt):
    model_gpt = model_gpt.to(device)
    optimizer_gpt = torch.optim.AdamW(model_gpt.parameters(), lr=gpt_config["min_lr"], betas=(gpt_config["beta1"], gpt_config["beta2"]), fused=True)
    scheduler_gpt = torch.optim.lr_scheduler.OneCycleLR(optimizer_gpt, max_lr=gpt_config["learning_rate"], steps_per_epoch=graph_creator.t_res * len(train_loader), epochs=gpt_config["max_iters"], pct_start=gpt_config["pct_start"], anneal_strategy='cos', final_div_factor=gpt_config["learning_rate"]/gpt_config["min_lr"])

if(gnn_config["compile"]):
    print("Compiling Models...")
    model_gnn = torch_geometric.compile(model_gnn, dynamic=True, fullgraph=True)
    if(use_gpt):
        model_gpt = torch.compile(model_gpt)

## Training
min_val_loss = 10e10
criterion = torch.nn.MSELoss(reduction="sum")
max_iters = gpt_config["max_iters"]

dateTimeObj = datetime.now()
timestring = f'{dateTimeObj.date().month}{dateTimeObj.date().day}{dateTimeObj.time().hour}{dateTimeObj.time().minute}'

save_path_gpt = f'models/GPT_{pde}_{gnn_config["experiment"]}_xresolution{gnn_config["base_resolution"][1]}-{gnn_config["super_resolution"][1]}_n{gnn_config["neighbors"]}_tw{gnn_config["time_window"]}_time{timestring}.pt'
save_path_gnn = f'models/GNN_{pde}_{gnn_config["experiment"]}_xresolution{gnn_config["base_resolution"][1]}-{gnn_config["super_resolution"][1]}_n{gnn_config["neighbors"]}_tw{gnn_config["time_window"]}_time{timestring}.pt'

save_path_log = f'logs/log_{pde}_{gnn_config["experiment"]}_xresolution{gnn_config["base_resolution"][1]}-{gnn_config["super_resolution"][1]}_n{gnn_config["neighbors"]}_tw{gnn_config["time_window"]}_time{timestring}.txt'

with open(save_path_log, 'w') as f:
    print(config, file=f)

for epoch in range(max_iters):
    print(f"Epoch {epoch}")
    train(gnn_config, pde, epoch, model_gnn, optimizer_gnn, train_loader, graph_creator, criterion, model_gpt, optimizer_gpt, scheduler_gpt, device=device)

    print("Evaluation on validation dataset:")
    val_loss = test(gnn_config, pde, model_gnn, valid_loader, graph_creator, criterion, model_gpt, device=device)
    wandb.log({
        "valid/loss": val_loss,
    })
    if(val_loss < min_val_loss):
        print("Evaluation on test dataset:")
        test_loss = test(gnn_config, pde, model_gnn, test_loader, graph_creator, criterion, model_gpt, device=device)

        wandb.log({
        "test/loss": test_loss,
        })
        # Save model
        torch.save(model_gnn.state_dict(), save_path_gnn)
        print(f"Saved model at {save_path_gnn}\n")
        if(use_gpt):
            torch.save(model_gpt.state_dict(), save_path_gpt)
            print(f"Saved model at {save_path_gpt}\n")
        min_val_loss = val_loss

    scheduler_gnn.step()
    wandb.log({
        "metrics/lr_gnn": scheduler_gnn.get_last_lr(),
    })

print(f"Test loss: {test_loss}")